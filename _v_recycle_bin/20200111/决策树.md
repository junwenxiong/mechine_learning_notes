# 决策树
## ID3算法(该算法只能用于离散型的数据)
* **具体方法**：
    * 1.从根结点(root node)开始，对结点计算所有可能的特征的信息增益，选择信息增益最大的特征作为结点的特征。
    
    * 2.由该特征的不同取值建立子节点，再对子结点递归地调用以上方法，构建决策树；直到所有特征的信息增益均很小或没有特征可以选择为止；
    
    * 3.最后得到一个决策树。
  
* **熵：**
    * 指的是所有类别所有可能值包含的信息期望值，可表示为：
    
    * ![](https://images2015.cnblogs.com/blog/771535/201508/771535-20150830102125625-984024386.jpg) 其中，p(xi​)是选择该分类的概率。

    * **熵越高，表明数据越不纯。**
  
* **信息增益：**
    * 1.概念：信息增益是相对于特征而言的。所以，特征A对训练数据集D的信息增益g(D,A)，定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差 即
        
    * 2.**公式：g(D,A)=H(D)−H(D∣A)**
   
    * 3.**我们要找出最好的信息增益，信息增益越大，则意味着使用属性a来进行划分所获得的“纯度提升”越大**



## C4.5算法
* C4.5对ID3的改进
    * 1.用信息增益率代替信息增益来选择属性，信息增益率=惩罚参数*信息增益
   
         ![信息增益比公式](_v_images/20200110203541822_2223.png) 
        * 注意：其中的HA(D)，对于样本集合D，将当前特征A作为随机变量（取值是特征A的各个特征值），求得的经验熵。

           
            <font color='red'> **（之前是把集合类别作为随机变量，现在把某个特征作为随机变量，按照此特征的特征取值对集合D进行划分，计算熵HA(D)）** </font>
            
         ![计算熵HA(D)公式](_v_images/20200110204411141_17985.png)
        *   <font color='red'>**信息增益率本质：是在信息增益的基础之上乘以一个惩罚参数。特征个数较多时，惩罚参数较小；特征个数较少时，惩罚参数较大**</font>
        * <font color='red'>**惩罚参数：**</font> **数据集D以特征A作为随机变量的熵的倒数，即：将特征A取值相同的样本划分到同一子集中**
            
           ![惩罚参数公式](_v_images/20200110212411315_27399.png)
           
        * **缺点：**
            * 信息增益率偏向取值较少的特征
            * **原因：** 当特征取值较少时HA(D)的值较少，则惩罚参数较大，因而信息增益率较大。因而偏向取值较少的特征。
        * <font color='red'>**使用信息增益率：**</font> **基于以上缺点，并不是直接选择信息增益率最大的特征，而是在候选特征中找出信息增益高于平均水平的特征，然后在这些特征中再选择信息增益率最高的特征**
    * 2.能够完成对连续值属性的离散化处理
    
         ![处理步骤](_v_images/20200109155027438_20735.png)
        
        ![计算公式](_v_images/20200110221646142_2503.png)
        
        * 选取信息增益最大的点，以该点作为划分点
    * 3.能处理属性值缺失的情况
        * 1.补平均值
        * 2.补众数
    * 4.在决策树构造完成之后进行剪枝
        * 1.过拟合现象 ，产生原因：有噪声和训练样本数太少了
        * 2.预剪枝：限制深度
        * 3.后剪枝：允许过度拟合，利用测试集对树进行修剪

## CART算法
* CART算法的定义：
    * **定义：** 基尼指数：表示在样本集合中一个随机选中的样本被分错的概率
        * <font color="red">**注意：**</font> Gini指数越小表示集合中被选中的样本被分错的概率越小，也就是说集合的纯度越高。
        
     * 基尼指数 = 样本被选中的概率 * 样本被分错的概率
     
     ![公式](_v_images/20200110224321867_750.png)
    
    * **说明：** 
         1. pk表示选中的样本属于k类别的概率，则这个样本被分错的概率是(1-pk)
         
         2. 样本集合中有K个类别，一个随机选中的样本可以属于这K个类别中的任意一个，取所有类别加起来之和
         
         3. 当为二分类时，Gini(P)=2p(1-p)
         
* **CART算法的特点**
    * CART是一棵**二叉树**，也就是说每个结点只能有两个分支，若下图第一种情况
    * CART是一棵**分类树**，则用**Gini指数**来确定分裂节点，而且Gini指数越小，分裂节点越纯
    
         ![第一种 离散值](_v_images/20200109173015239_18655.png)

         * 上图中婚姻状况属性有三个特征值，因而有三种划分的可能，划分如上图所示
         
         ![划分公式](_v_images/20200110225758738_22650.png)
    * CART 是一棵**回归树**的话，则用**最小方差**确定分裂节点，方差越小，分裂节点越纯
      
      ![第二种 连续值](_v_images/20200109172714909_21113.png)
    
     * 对特征值进行排序，两两相邻的值求平均值，对每个平均值求最小方差，找方差最小的值
    

* CART剪枝
    * ![步骤1](_v_images/20200109220107581_25663.png)
    * ![步骤2](_v_images/20200109220127389_4401.png)
    * ![公式](_v_images/20200109220001243_6366.png)
    * ![计算过程](_v_images/20200109221057486_22925.png)

## CHAID决策树
![1](_v_images/20200110231802676_20166.png)


![2](_v_images/20200110231835123_31842.png)

* **n为a+b+c+d**

![3](_v_images/20200110120549466_10741.png)

![4](_v_images/20200110231718768_29886.png)



## 决策树总结
* **决策树的特点：**
    * ![特点1](_v_images/20200110171629184_30222.png)
    * ![特点2](_v_images/20200110171816447_5502.png)
 
* **决策树的优点**：
    * 1.推理过程容易理解，推理过程可以表示成If Then 形式：
    
    * 2.推理过程完全依赖于属性变量的取值特点：
    
    * 3.判断属性变量的重要性，减少变量的数目
* ![优点1](_v_images/20200110172024700_13286.png)

* **决策树的优势**：
    * 1.之所以被选用是因为它能理顺一些可以理解的规则，但也有可能发生过拟合现象：
    
    * 2.回归模型的回归系数具有可解释性，在流行病研究中，对治病因素的效应，常用一些危险程度指标来衡量因素与发病的联系程度。